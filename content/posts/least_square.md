---
title: 最小二乘法和线性回归
author: pigLoveRabbit
tags: []
categories:
  - Machine Learning
date: 2024-10-08 14:00:00
---
## 最小二乘法
早在19世纪,勒让德就认为让"误差的平方和最小"估计出来的模型是最接近真实情形的。  
按照勒让德的最佳原则,于是就是求:  
$$
\text{L} = \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2
$$
这个目标函数取得最小值时的函数参数,这就是最小二乘法的思想想,所谓"二乘"就是平方的意思。从这里我们可以看到,**最小二乘法其实
就是用来做函数拟合的一种思想**。  
至于怎么求出具体的参数那就是另外一个问题了,理论上可以用导数法、几何法,工程上可以用**梯度下降法**。下面以最常用的线性回归为
例进行推导和理解。  
在**机器学习**中用于回归问题的损失函数(Loss Function)是均方误差(MSE)：
$$
\text{L} = \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2
$$
其实就是多了个1/2n。

## 线性回归
线性回归
线性回归因为比较简单,可以直接推导出解析解,而且许多非线性的问题也可以转化为线性问题来解决,所以得到了广泛的应用。甚至许多人认为最小二乘法指的就是线性回归,其实并不是,最小二乘法就是一种思想,它可以拟合任意函数,线性回归只是其中一个比较简单而且也很常用的函数,所以讲最小二乘法基本都会以它为例。  
下面我会先用矩阵法进行推导,然后再用几何法来帮助你理解最小二乘法的几何意义。


